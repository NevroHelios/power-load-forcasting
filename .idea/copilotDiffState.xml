<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/eda.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/eda.ipynb" />
              <option name="originalContent" value="#%% md&#10;# Imports&#10;#%%&#10;import sklearn&#10;import pandas as pd&#10;import requests&#10;import seaborn as sns&#10;import matplotlib.pyplot as plt&#10;import warnings&#10;import holidays&#10;import numpy as np&#10;&#10;from seaborn import kdeplot&#10;from sklearn.linear_model import Ridge&#10;from sklearn.ensemble import GradientBoostingClassifier&#10;from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report&#10;from sklearn.impute import SimpleImputer&#10;from sklearn.model_selection import GridSearchCV&#10;from sklearn.preprocessing import StandardScaler&#10;import xgboost as xgb&#10;from xgboost import XGBClassifier&#10;&#10;warnings.filterwarnings('ignore')&#10;warnings.filterwarnings('always')&#10;sns.set_style('darkgrid')&#10;plt.style.use('dark_background')&#10;#%%&#10;df = pd.read_csv(&quot;data/load_data.csv&quot;)&#10;df.head()&#10;#%% md&#10;# Data Cleaning&#10;#%%&#10;df.isnull().sum()&#10;#%%&#10;df['Date_Time'] = pd.to_datetime(df['Date_Time'], format=&quot;%d-%m-%Y %H:%M&quot;)&#10;&#10;# train, text marking&#10;df['train'] = 1&#10;df.loc[df['Date_Time'].dt.month == 12, 'train' ] = 0&#10;&#10;df.reset_index(drop=True)&#10;#%%&#10;class CFG:&#10;    sinusol_hours = ['hour_sin', 'hour_cos', 'hour_sin 2t', 'hour_cos 2t', 'hour_sin 3t', 'hour_cos 3t','hour_sin 4t', 'hour_cos 4t', 'hour_sin t/2', 'hour_cos t/2']&#10;    sinusol_months = ['month_sin', 'month_cos',]&#10;    sinsols = [*sinusol_months, *sinusol_hours]&#10;&#10;# convert Date_Time to datetime and extract month and day&#10;df['month'] = df['Date_Time'].dt.month&#10;df['daynum'] = (df.Date_Time - df.Date_Time.iloc[0]).dt.days&#10;df['weekday'] = df.Date_Time.dt.weekday&#10;# no need for year as all are from 2018&#10;df['dayofmonth'] = df.Date_Time.dt.day&#10;df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)&#10;df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)&#10;df['hour'] = df.Date_Time.dt.hour&#10;df['hour_sin t/2'] = np.sin(2 * np.pi * df['hour'] / 24)&#10;df['hour_cos t/2'] = np.cos(2 * np.pi * df['hour'] / 24)&#10;df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)&#10;df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)&#10;df['hour_sin 2t'] = np.sin(4 * np.pi * df['hour'] / 24)&#10;df['hour_cos 2t'] = np.cos(4 * np.pi * df['hour'] / 24)&#10;df['hour_sin 3t'] = np.sin(6 * np.pi * df['hour'] / 24)&#10;df['hour_cos 3t'] = np.cos(6 * np.pi * df['hour'] / 24)&#10;df['hour_sin 4t'] = np.sin(8 * np.pi * df['hour'] / 24)&#10;df['hour_cos 4t'] = np.cos(8 * np.pi * df['hour'] / 24)&#10;df['dayofweek_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)&#10;df['dayofweek_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)&#10;df&#10;#%% md&#10;# Impute missing values&#10;#%%&#10;df.columns&#10;#%%&#10;usage_mean_per_month = df.groupby(['month', 'Load_Type'])['Usage_kWh'].mean().reset_index()&#10;lag_curr_reactive_mean_per_month = df.groupby(['month', 'Load_Type'])['Lagging_Current_Reactive.Power_kVarh'].mean().reset_index()&#10;lead_curr_reactive_mean_per_month = df.groupby(['month', 'Load_Type'])['Leading_Current_Reactive_Power_kVarh'].mean().reset_index()&#10;co2_mean_per_month = df.groupby(['month', 'Load_Type'])['CO2(tCO2)'].mean().reset_index()&#10;lagging_curr_pf_mean_per_month = df.groupby(['month', 'Load_Type'])['Lagging_Current_Power_Factor'].mean().reset_index()&#10;leading_curr_pf_mean_per_month = df.groupby(['month', 'Load_Type'])['Leading_Current_Power_Factor'].mean().reset_index()&#10;&#10;df['Usage_kWh'].fillna(df.merge(usage_mean_per_month, on=['month', 'Load_Type'], how='left')['Usage_kWh_y'], inplace=True)&#10;df['Lagging_Current_Reactive.Power_kVarh'].fillna(df.merge(lag_curr_reactive_mean_per_month, on=['month', 'Load_Type'], how='left')['Lagging_Current_Reactive.Power_kVarh_y'], inplace=True)&#10;df['Leading_Current_Reactive_Power_kVarh'].fillna(df.merge(lead_curr_reactive_mean_per_month, on=['month', 'Load_Type'], how='left')['Leading_Current_Reactive_Power_kVarh_y'], inplace=True)&#10;df['CO2(tCO2)'].fillna(df.merge(co2_mean_per_month, on=['month', 'Load_Type'], how='left')['CO2(tCO2)_y'], inplace=True)&#10;df['Lagging_Current_Power_Factor'].fillna(df.merge(lagging_curr_pf_mean_per_month, on=['month', 'Load_Type'], how='left')['Lagging_Current_Power_Factor_y'], inplace=True)&#10;df['Leading_Current_Power_Factor'].fillna(df.merge(leading_curr_pf_mean_per_month, on=['month', 'Load_Type'], how='left')['Leading_Current_Power_Factor_y'], inplace=True)&#10;&#10;df.drop(['NSM'], inplace=True, axis=1, errors='ignore')&#10;&#10;df['total_reactive_power'] = df['Lagging_Current_Reactive.Power_kVarh'] + df['Leading_Current_Reactive_Power_kVarh']&#10;df['power_factor_diff'] = df['Lagging_Current_Power_Factor'] - df['Leading_Current_Power_Factor']&#10;df['reactive_usage_ratio'] = df['total_reactive_power'] / (df['Usage_kWh'] + 0.00001)&#10;&#10;total_load_time_per_day = df['Load_Type']&#10;#%% md&#10;# EDA&#10;#%%&#10;df.groupby('month')['Load_Type'].value_counts().unstack().plot(kind='bar', figsize=(12, 6))&#10;#%%&#10;df['Load_Type'].unique()&#10;#%%&#10;load_count_per_day = df.groupby('daynum')['Load_Type'].value_counts().reset_index()&#10;load_count_per_day['percent_a_day_y'] = load_count_per_day['count']*15 / (60*24)&#10;load_count_per_day&#10;#%%&#10;df['load_percent_a_day'] = df.merge(load_count_per_day, on=['daynum', 'Load_Type'], how='left')['percent_a_day_y']&#10;df&#10;#%%&#10;for load in df['Load_Type'].unique():&#10;    abt = df.groupby('hour')&#10;    x = abt[CFG.sinusol_hours].mean().to_numpy()&#10;    y = abt['load_percent_a_day'].mean().to_numpy()&#10;&#10;    reg = Ridge(alpha=0.1).fit(x, y)&#10;    p = reg.predict(x)&#10;&#10;    plt.plot(y, 'b')&#10;    plt.plot(p, 'r')&#10;&#10;plt.show()&#10;#%%&#10;df['isHoliday'] = 0&#10;for holiday, _ in holidays.CountryHoliday('IND', years=2018).items():&#10;    df.loc[(df.Date_Time.dt.day == holiday.day) &amp; (df.Date_Time.dt.month == holiday.month), 'isHoliday'] = 1&#10;&#10;season_map = {&#10;    12: 'Winter', 1: 'Winter', 2: 'Winter',&#10;    3: 'Spring', 4: 'Spring', 5: 'Spring',&#10;    6: 'Summer', 7: 'Summer', 8: 'Summer',&#10;    9: 'Fall', 10: 'Fall', 11: 'Fall'&#10;}&#10;df['season'] = df['month'].map(season_map)&#10;&#10;df['usage_hour_sin'] = df['Usage_kWh'] * df['hour_sin']&#10;df['usage_hour_cos'] = df['Usage_kWh'] * df['hour_cos']&#10;&#10;df['usage_rolling_mean_3h'] = df['Usage_kWh'].rolling(12).mean()# data in 15 min so -&gt; 15 * 12&#10;df['usage_rolling_mean_6h'] = df['Usage_kWh'].rolling(24).mean()&#10;# drop the first NaN values&#10;df.dropna(axis=0, inplace=True)&#10;#%%&#10;df.drop(['day', 'load_factor', 'daynum', 'load_percent_a_day', 'month', 'season'], inplace=True, axis=1, errors='ignore')&#10;#%%&#10;df.columns&#10;#%%&#10;load_map = {&#10;    'Light_Load': 0,&#10;    'Medium_Load': 1,&#10;    'Maximum_Load': 2&#10;}&#10;rev_load = {&#10;    0: 'Light_Load',&#10;    1: 'Medium_Load',&#10;    2: 'Maximum_Load'&#10;}&#10;&#10;df['Load_Type'] = df['Load_Type'].map(load_map)&#10;scalar = StandardScaler()&#10;&#10;train = df[df['train'] == 1].drop('Date_Time',axis=1)&#10;test = df[df['train'] == 0].drop('Date_Time', axis=1)&#10;y_train = train['Load_Type']&#10;x_train = train.drop(['Load_Type'], axis=1)&#10;y_test = test['Load_Type']&#10;x_test = test.drop(['Load_Type'], axis=1)&#10;&#10;x_train_scaled = scalar.fit_transform(x_train)&#10;x_test_scaled = scalar.transform(x_test)&#10;&#10;x_train.shape, y_train.shape, x_test.shape, y_test.shape&#10;#%%&#10;gb_model = GradientBoostingClassifier(&#10;    loss='log_loss',&#10;    learning_rate=0.1,&#10;    n_estimators=250,&#10;    subsample=1.0,&#10;    criterion='friedman_mse',&#10;    min_samples_split=5,&#10;    min_samples_leaf=1,&#10;    min_weight_fraction_leaf=0.0,&#10;    max_depth=5,&#10;    min_impurity_decrease=0.0,&#10;    init=None,&#10;    random_state=42,&#10;    max_features=None,&#10;    verbose=0,&#10;    max_leaf_nodes=None,&#10;    warm_start=False,&#10;    validation_fraction=0.1,&#10;    n_iter_no_change=None,&#10;    tol=1e-4,&#10;    ccp_alpha=0.0&#10;)&#10;gb_model.fit(x_train, y_train)&#10;#%%&#10;def results(y_true, y_hat):&#10;    print(f&quot;Accuracy: {accuracy_score(y_true, y_hat)}&quot;)&#10;    print(f&quot;F1 Score: {f1_score(y_true, y_hat, average='macro')}&quot;)&#10;    report = classification_report(y_true, y_hat)&#10;    print(report)&#10;&#10;results(y_test, gb_model.predict(x_test))&#10;#%%&#10;param_grid = {&#10;    'max_depth': [7], # [3, 5, 7, 9],&#10;    'min_child_weight': [5],# [1, 3, 5],&#10;    'gamma': [0.3], # [0, 0.1, 0.3],&#10;    'subsample': [1.0], # [0.8, 1.0],&#10;    'colsample_bytree': [1.0], # [0.8, 1.0],&#10;    'learning_rate': [0.1],&#10;    'n_estimators': [50, 100, 300, 500],&#10;    'scale_pos_weight': [1, 2, 3]  # especially if imbalanced&#10;}&#10;#%%&#10;xgb_model = xgb.XGBClassifier()&#10;grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=2)&#10;grid_search.fit(x_train, y_train)&#10;#%%&#10;xgb_model = XGBClassifier(**grid_search.best_params_)&#10;xgb_model.fit(x_train, y_train)&#10;#%%&#10;results(y_test, grid_search.predict(x_test))&#10;#%%&#10;from lightgbm import LGBMClassifier&#10;&#10;lgbm = LGBMClassifier(&#10;    objective='multiclass',&#10;    num_class=3,&#10;    learning_rate=0.05,&#10;    max_depth=7,&#10;    n_estimators=300,&#10;    subsample=1,&#10;    colsample_bytree=0.9,&#10;    class_weight='balanced',  # try this if imbalance exists&#10;    random_state=42,&#10;).fit(x_train, y_train)&#10;&#10;results(y_test, lgbm.predict(x_test))&#10;#%%&#10;confusion_matrix(y_test, grid_search.predict(x_test))" />
              <option name="updatedContent" value="#%% md&#10;# Imports&#10;#%%&#10;import sklearn&#10;import pandas as pd&#10;import requests&#10;import seaborn as sns&#10;import matplotlib.pyplot as plt&#10;import warnings&#10;import holidays&#10;import numpy as np&#10;&#10;from seaborn import kdeplot&#10;from sklearn.linear_model import Ridge&#10;from sklearn.ensemble import GradientBoostingClassifier&#10;from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report&#10;from sklearn.impute import SimpleImputer&#10;from sklearn.model_selection import GridSearchCV&#10;from sklearn.preprocessing import StandardScaler&#10;import xgboost as xgb&#10;from xgboost import XGBClassifier&#10;&#10;warnings.filterwarnings('ignore')&#10;warnings.filterwarnings('always')&#10;sns.set_style('darkgrid')&#10;plt.style.use('dark_background')&#10;#%%&#10;df = pd.read_csv(&quot;data/load_data.csv&quot;)&#10;df.head()&#10;#%% md&#10;# Data Cleaning&#10;#%%&#10;df.isnull().sum()&#10;#%%&#10;df['Date_Time'] = pd.to_datetime(df['Date_Time'], format=&quot;%d-%m-%Y %H:%M&quot;)&#10;&#10;# train, text marking&#10;df['train'] = 1&#10;df.loc[df['Date_Time'].dt.month == 12, 'train' ] = 0&#10;&#10;df.reset_index(drop=True)&#10;#%%&#10;class CFG:&#10;    sinusol_hours = ['hour_sin', 'hour_cos', 'hour_sin 2t', 'hour_cos 2t', 'hour_sin 3t', 'hour_cos 3t','hour_sin 4t', 'hour_cos 4t', 'hour_sin t/2', 'hour_cos t/2']&#10;    sinusol_months = ['month_sin', 'month_cos',]&#10;    sinsols = [*sinusol_months, *sinusol_hours]&#10;&#10;# convert Date_Time to datetime and extract month and day&#10;df['month'] = df['Date_Time'].dt.month&#10;df['daynum'] = (df.Date_Time - df.Date_Time.iloc[0]).dt.days&#10;df['weekday'] = df.Date_Time.dt.weekday&#10;# no need for year as all are from 2018&#10;df['dayofmonth'] = df.Date_Time.dt.day&#10;df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)&#10;df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)&#10;df['hour'] = df.Date_Time.dt.hour&#10;df['hour_sin t/2'] = np.sin(2 * np.pi * df['hour'] / 24)&#10;df['hour_cos t/2'] = np.cos(2 * np.pi * df['hour'] / 24)&#10;df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)&#10;df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)&#10;df['hour_sin 2t'] = np.sin(4 * np.pi * df['hour'] / 24)&#10;df['hour_cos 2t'] = np.cos(4 * np.pi * df['hour'] / 24)&#10;df['hour_sin 3t'] = np.sin(6 * np.pi * df['hour'] / 24)&#10;df['hour_cos 3t'] = np.cos(6 * np.pi * df['hour'] / 24)&#10;df['hour_sin 4t'] = np.sin(8 * np.pi * df['hour'] / 24)&#10;df['hour_cos 4t'] = np.cos(8 * np.pi * df['hour'] / 24)&#10;df['dayofweek_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)&#10;df['dayofweek_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)&#10;df&#10;#%% md&#10;# Impute missing values&#10;#%%&#10;df.columns&#10;#%%&#10;usage_mean_per_month = df.groupby(['month', 'Load_Type'])['Usage_kWh'].mean().reset_index()&#10;lag_curr_reactive_mean_per_month = df.groupby(['month', 'Load_Type'])['Lagging_Current_Reactive.Power_kVarh'].mean().reset_index()&#10;lead_curr_reactive_mean_per_month = df.groupby(['month', 'Load_Type'])['Leading_Current_Reactive_Power_kVarh'].mean().reset_index()&#10;co2_mean_per_month = df.groupby(['month', 'Load_Type'])['CO2(tCO2)'].mean().reset_index()&#10;lagging_curr_pf_mean_per_month = df.groupby(['month', 'Load_Type'])['Lagging_Current_Power_Factor'].mean().reset_index()&#10;leading_curr_pf_mean_per_month = df.groupby(['month', 'Load_Type'])['Leading_Current_Power_Factor'].mean().reset_index()&#10;&#10;df['Usage_kWh'].fillna(df.merge(usage_mean_per_month, on=['month', 'Load_Type'], how='left')['Usage_kWh_y'], inplace=True)&#10;df['Lagging_Current_Reactive.Power_kVarh'].fillna(df.merge(lag_curr_reactive_mean_per_month, on=['month', 'Load_Type'], how='left')['Lagging_Current_Reactive.Power_kVarh_y'], inplace=True)&#10;df['Leading_Current_Reactive_Power_kVarh'].fillna(df.merge(lead_curr_reactive_mean_per_month, on=['month', 'Load_Type'], how='left')['Leading_Current_Reactive_Power_kVarh_y'], inplace=True)&#10;df['CO2(tCO2)'].fillna(df.merge(co2_mean_per_month, on=['month', 'Load_Type'], how='left')['CO2(tCO2)_y'], inplace=True)&#10;df['Lagging_Current_Power_Factor'].fillna(df.merge(lagging_curr_pf_mean_per_month, on=['month', 'Load_Type'], how='left')['Lagging_Current_Power_Factor_y'], inplace=True)&#10;df['Leading_Current_Power_Factor'].fillna(df.merge(leading_curr_pf_mean_per_month, on=['month', 'Load_Type'], how='left')['Leading_Current_Power_Factor_y'], inplace=True)&#10;&#10;df.drop(['NSM'], inplace=True, axis=1, errors='ignore')&#10;&#10;df['total_reactive_power'] = df['Lagging_Current_Reactive.Power_kVarh'] + df['Leading_Current_Reactive_Power_kVarh']&#10;df['power_factor_diff'] = df['Lagging_Current_Power_Factor'] - df['Leading_Current_Power_Factor']&#10;df['reactive_usage_ratio'] = df['total_reactive_power'] / (df['Usage_kWh'] + 0.00001)&#10;&#10;total_load_time_per_day = df['Load_Type']&#10;#%% md&#10;# EDA&#10;#%%&#10;df.groupby('month')['Load_Type'].value_counts().unstack().plot(kind='bar', figsize=(12, 6))&#10;#%%&#10;df['Load_Type'].unique()&#10;#%%&#10;load_count_per_day = df.groupby('daynum')['Load_Type'].value_counts().reset_index()&#10;load_count_per_day['percent_a_day_y'] = load_count_per_day['count']*15 / (60*24)&#10;load_count_per_day&#10;#%%&#10;df['load_percent_a_day'] = df.merge(load_count_per_day, on=['daynum', 'Load_Type'], how='left')['percent_a_day_y']&#10;df&#10;#%%&#10;for load in df['Load_Type'].unique():&#10;    abt = df.groupby('hour')&#10;    x = abt[CFG.sinusol_hours].mean().to_numpy()&#10;    y = abt['load_percent_a_day'].mean().to_numpy()&#10;&#10;    reg = Ridge(alpha=0.1).fit(x, y)&#10;    p = reg.predict(x)&#10;&#10;    plt.plot(y, 'b')&#10;    plt.plot(p, 'r')&#10;&#10;plt.show()&#10;#%%&#10;df['isHoliday'] = 0&#10;for holiday, _ in holidays.CountryHoliday('IND', years=2018).items():&#10;    df.loc[(df.Date_Time.dt.day == holiday.day) &amp; (df.Date_Time.dt.month == holiday.month), 'isHoliday'] = 1&#10;&#10;season_map = {&#10;    12: 'Winter', 1: 'Winter', 2: 'Winter',&#10;    3: 'Spring', 4: 'Spring', 5: 'Spring',&#10;    6: 'Summer', 7: 'Summer', 8: 'Summer',&#10;    9: 'Fall', 10: 'Fall', 11: 'Fall'&#10;}&#10;df['season'] = df['month'].map(season_map)&#10;&#10;df['usage_hour_sin'] = df['Usage_kWh'] * df['hour_sin']&#10;df['usage_hour_cos'] = df['Usage_kWh'] * df['hour_cos']&#10;&#10;df['usage_rolling_mean_3h'] = df['Usage_kWh'].rolling(12).mean()# data in 15 min so -&gt; 15 * 12&#10;df['usage_rolling_mean_6h'] = df['Usage_kWh'].rolling(24).mean()&#10;# drop the first NaN values&#10;df.dropna(axis=0, inplace=True)&#10;#%%&#10;df.drop(['day', 'load_factor', 'daynum', 'load_percent_a_day', 'month', 'season'], inplace=True, axis=1, errors='ignore')&#10;#%%&#10;df.columns&#10;#%%&#10;load_map = {&#10;    'Light_Load': 0,&#10;    'Medium_Load': 1,&#10;    'Maximum_Load': 2&#10;}&#10;rev_load = {&#10;    0: 'Light_Load',&#10;    1: 'Medium_Load',&#10;    2: 'Maximum_Load'&#10;}&#10;&#10;df['Load_Type'] = df['Load_Type'].map(load_map)&#10;scalar = StandardScaler()&#10;&#10;train = df[df['train'] == 1].drop('Date_Time',axis=1)&#10;test = df[df['train'] == 0].drop('Date_Time', axis=1)&#10;y_train = train['Load_Type']&#10;x_train = train.drop(['Load_Type'], axis=1)&#10;y_test = test['Load_Type']&#10;x_test = test.drop(['Load_Type'], axis=1)&#10;&#10;x_train_scaled = scalar.fit_transform(x_train)&#10;x_test_scaled = scalar.transform(x_test)&#10;&#10;x_train.shape, y_train.shape, x_test.shape, y_test.shape&#10;#%%&#10;gb_model = GradientBoostingClassifier(&#10;    loss='log_loss',&#10;    learning_rate=0.1,&#10;    n_estimators=250,&#10;    subsample=1.0,&#10;    criterion='friedman_mse',&#10;    min_samples_split=5,&#10;    min_samples_leaf=1,&#10;    min_weight_fraction_leaf=0.0,&#10;    max_depth=5,&#10;    min_impurity_decrease=0.0,&#10;    init=None,&#10;    random_state=42,&#10;    max_features=None,&#10;    verbose=0,&#10;    max_leaf_nodes=None,&#10;    warm_start=False,&#10;    validation_fraction=0.1,&#10;    n_iter_no_change=None,&#10;    tol=1e-4,&#10;    ccp_alpha=0.0&#10;)&#10;gb_model.fit(x_train, y_train)&#10;#%%&#10;def results(y_true, y_hat):&#10;    print(f&quot;Accuracy: {accuracy_score(y_true, y_hat)}&quot;)&#10;    print(f&quot;F1 Score: {f1_score(y_true, y_hat, average='macro')}&quot;)&#10;    report = classification_report(y_true, y_hat)&#10;    print(report)&#10;&#10;results(y_test, gb_model.predict(x_test))&#10;#%%&#10;param_grid = {&#10;    'max_depth': [7], # [3, 5, 7, 9],&#10;    'min_child_weight': [5],# [1, 3, 5],&#10;    'gamma': [0.3], # [0, 0.1, 0.3],&#10;    'subsample': [1.0], # [0.8, 1.0],&#10;    'colsample_bytree': [1.0], # [0.8, 1.0],&#10;    'learning_rate': [0.1],&#10;    'n_estimators': [50, 100, 300, 500],&#10;    'scale_pos_weight': [1, 2, 3]  # especially if imbalanced&#10;}&#10;#%%&#10;xgb_model = xgb.XGBClassifier()&#10;grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=2)&#10;grid_search.fit(x_train, y_train)&#10;#%%&#10;xgb_model = XGBClassifier(**grid_search.best_params_)&#10;xgb_model.fit(x_train, y_train)&#10;#%%&#10;results(y_test, grid_search.predict(x_test))&#10;#%%&#10;from lightgbm import LGBMClassifier&#10;&#10;lgbm = LGBMClassifier(&#10;    objective='multiclass',&#10;    num_class=3,&#10;    learning_rate=0.05,&#10;    max_depth=7,&#10;    n_estimators=300,&#10;    subsample=1,&#10;    colsample_bytree=0.9,&#10;    class_weight='balanced',  # try this if imbalance exists&#10;    random_state=42,&#10;).fit(x_train, y_train)&#10;&#10;results(y_test, lgbm.predict(x_test))&#10;#%%&#10;confusion_matrix(y_test, grid_search.predict(x_test))" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>